<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-04-13T16:49:08-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Aditi Kabra</title><subtitle>Aditi Kabra</subtitle><entry><title type="html">Testing frameworks for a decoupled Frontend and Backend</title><link href="http://localhost:4000/testing/2020/04/13/testing.html" rel="alternate" type="text/html" title="Testing frameworks for a decoupled Frontend and Backend" /><published>2020-04-13T00:00:00-07:00</published><updated>2020-04-13T00:00:00-07:00</updated><id>http://localhost:4000/testing/2020/04/13/testing</id><content type="html" xml:base="http://localhost:4000/testing/2020/04/13/testing.html">&lt;p&gt;I am faced with the interesting task of designing an effective testing system for a web service with a decoupled back end and frontend, and multiple dependencies on other services. This post is about the interesting design and implementation questions that arise.&lt;/p&gt;

&lt;p&gt;The ultimate vision (due to my amazing manager) is to create a framework capable of automating the running of end to end test suites on scenarios deduced from logs. This way, when a failure occurs in the live system, it would be easy to drill down to the cause by running the tests generating mock dependency responses from the logs.
Further, the logs could then be easily modified into regression tests.&lt;/p&gt;

&lt;p&gt;As a first step, I identify the requirements of the framework, and what I would like it to be able to do.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Scenario 1: Test driven development&lt;/p&gt;

    &lt;p&gt;I want it to be easy to write new tests and modify tests when developing new features.
  This suggests that&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;There should be an easy, simplistic way to inject handwritten responses into mocks. Given the fact that it should also be possible to read from logs, I see some different potential designs:
        &lt;ul&gt;
          &lt;li&gt;mocks could listen to “eventHandlers” that hand in data to the mock, separate implementations running depending on what the source of behavior of the mocks is.&lt;/li&gt;
          &lt;li&gt;mocks could depend on and interface for “dataProviders”, which similarly read from different sources.&lt;/li&gt;
          &lt;li&gt;Logs could be compiled down to a format that the mocks would read.
The advantages and disadvantages of each pattern will be discussed later.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Reusability ought to be built into the system. A change in the contract of one piece somewhere in the system or its dependencies should require only a minimal number of changes in testing code.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Scenario 2: Debugging&lt;/p&gt;

    &lt;p&gt;This testing framework has the potential to make debugging much easier. Influenced by an &lt;a href=&quot;https://hackernoon.com/how-to-debug-any-problem-ac6f8a867fae&quot;&gt;article on effective debugging&lt;/a&gt;, I list the things I would like my framework to be able to support.&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;To be able to quickly identify what still works, and what doesn’t.
I think of the system in terms of length up and down the stack, and breadth across the processes in the service. I want the framework to be able to slice and dice either.&lt;/p&gt;

        &lt;p&gt;To “dice”, or cut across the length of the system and test a smaller part of the stack, I would like mocking at multiple levels. If class Controller, further down the stack, depends on class Manager, which in turn depends on DataClient, I want to have mocks for all of these. I want to say, for example, that when the Manager is mocked, things still work, so the problem must lie at a lower level in the logic. When I mock the storage client things break, so the problem lies in the Manager’s logic.&lt;/p&gt;

        &lt;p&gt;To “slice” or cut away the breadth of the system, I would like to be able to mock some components but not others. For example, If class Controller depends on two Managers, I’d like to be able to mock one but not the other.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;To be able to simplify the tests easily
Starting from a complex failure scenario, I would like it to be possible to easily modify the inputs to get to the simplest case where the error still occurs.
What this amounts to is it should be easy to put replace some mocks with the “simple” scenario result, while retaining complexity for other pieces.
One solution that this suggests is some sort of compilation from logs to test files, since modifying the logs would be a lot harder than modifying a format specifically meant for the mocks.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Enable hypothesis driven search for the problem.
I think what is required to support this overlaps with the features already discussed- the ability to slice and dice across the system, and easily modify and simplify tests.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Generating anti-regression tests
This also could be done with a combination of log-read mock generation.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Other general requirements&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;The one thing that must absolutely not happen is having the   mocks somehow getting called in the production system.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;All mocks should be in a separate project, away from the production system. The ideal solution is a separate test host could take in the production system as references and inject mocks.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Running tests across a decoupled back end and front end.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;There ought to be one place where execution stars, that owns overarching control during a test, it should be possible to control both the back end and the UX from this place.
&lt;!-- With a single action I would like it to be possible to start both the front end and the back end. --&gt;
If this “place” is in the UX test suite, then which mocks are to be used could be communicated via test headers and flags. The flags would mean nothing if they hit a production end point, and would only be accounted for in the test host.
If the “place” lies in the back end test host then it should be able to call upon the UX testing framework (I’m going to use selenium).
The latter has the advantage that the setup would all happen in one place, one would not need to startup the back end test host and UX hosting separately. There would also be no overpowered flags in the url.
It is important to note that whichever way I proceed, I want to be able to leverage my IDE’s ability to set breakpoints. In my case, with Visual Studio I can debug across multiple projects so life is easy.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With these requirements laid out, I go on on to take a stab at the questions that arise.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;So, what pattern do we follow to get have the ability to switch out mock data sources?
    &lt;ul&gt;
      &lt;li&gt;I see the problem with an event handlers to be that they are too loosely tied into the application. While running the program you simply trust that they will pick things up. Further, I’m using C# and I suspect that dependency injection would be more idiomatic and better supported. An advantage is that in applications like logging, treating the event handling as externally attached from a separate testing interface might be the right way to think of things.&lt;/li&gt;
      &lt;li&gt;I’m inclined to use dependency injection and have ways to re-register different mock readers. I think that this allows for the flexibility of adding more ways to put in mock data while still having the robustness advantages of a hard dependency.&lt;/li&gt;
      &lt;li&gt;Having a transpiler from logs might still be useful. The intermediate language files could cover the easy-to-edit requirements. Not only would an intermediate language make it easy to edit tests, but also automatic test generation would make it possible to enforce the correct reusing of testing code during test generation so that it would be easier to maintain tests in the event of contract changes.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;How do we bundle our two services into a single-click start while still leveraging IDE debugging&lt;/p&gt;

    &lt;p&gt;There are three processes that need to be running during test runs. (1) The API server (2) the front end (side loaded) (3) the tests. While (3) should be hosted by the IDE while debugging locally, the situation may be different for continuous integration. So the project that we are testing must set the other two up and running.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The aim of this post was to share ideas about what I think can be a very powerful testing framework, in the hope that other engineers trying to solve a similar problem may find some useful ideas, or in turn have suggestions. I will update what I have written if in the process of implementing or maintaining, I come across more points of interest.&lt;/p&gt;</content><author><name></name></author><summary type="html">I am faced with the interesting task of designing an effective testing system for a web service with a decoupled back end and frontend, and multiple dependencies on other services. This post is about the interesting design and implementation questions that arise.</summary></entry><entry><title type="html">Literature Reviews</title><link href="http://localhost:4000/literature/2020/04/12/literature-review.html" rel="alternate" type="text/html" title="Literature Reviews" /><published>2020-04-12T09:07:06-07:00</published><updated>2020-04-12T09:07:06-07:00</updated><id>http://localhost:4000/literature/2020/04/12/literature-review</id><content type="html" xml:base="http://localhost:4000/literature/2020/04/12/literature-review.html">&lt;!-- #### This is a draft. I will keep appending, but publishing now anyway so I have incentive to complete and because no one's going to land here anyway lol --&gt;

&lt;p&gt;As a beginning researcher, something I have had to do often is literature reviews. Whether I’m to track down all relevant existing work at the start of a new project, or am trying to find answers to a specific question that I know had to have been answered at some point in the past, I find myself requiring the ability to effectively search through the existing body of academic work.
&lt;!-- It's one of the things that I was rather bad at to begin with, that I was never taught how to do right in classes.  --&gt;
This post is intended for the benefit of those who find themselves in the same place I was an year ago- faced with the task of having to search through dense literature in a field they are not familiar with, sans guidance, with no idea where to start.&lt;/p&gt;

&lt;!-- I'm going to walk through examples how I go about searching for relevant papers.
First, I attempt to perform a broad field review in an area I have no familiarity with- mathematical sociology. My objective is to identify seminal papers and get a rough feel for the shape of the field. 
In the second example, I search for a specific piece of information- the exact reduction of the computation of the Hafnian fo a matrix to a classic sharp-P complete problem (more context on this later), assuming that I have no familiarity with the existing literature of complexity theory and only an undergraduate's course-based understanding of the field. --&gt;

&lt;p&gt;I’m going to walk through an example of how I go about searching for relevant papers.
Here, I attempt to perform a broad field review in an area I have no familiarity with- mathematical sociology. My objective is to identify seminal papers and get a rough feel for the shape of the field.&lt;/p&gt;

&lt;p&gt;Disclaimer: This article is based on the tricks I’ve picked up through trial and error over the course of my limited experience in research. I’m sure there’s a lot I don’t know, so if you have suggestions/advice, I’d love to hear it!&lt;/p&gt;

&lt;h4 id=&quot;surveying-a-field&quot;&gt;Surveying a Field&lt;/h4&gt;

&lt;p&gt;I aim to survey mathematical sociology, starting off with only the vague notion that sociology is the study of society and using math to formalize it might be interesting. The objective is to get a rough sense of the main ideas in the field. Some things to look out for as I walk through the process is how I find starting points, and how I use one paper to lead into identifying others of interest.
I need to find a entry point to jump in, so I pay a visit to the hallowed halls of &lt;a href=&quot;https://en.wikipedia.org/wiki/Mathematical_sociology&quot;&gt;Wikipedia&lt;/a&gt;.
&lt;!-- I also search &quot;mathematical sociology survey&quot; on google scholar. --&gt;&lt;/p&gt;

&lt;p&gt;Skimming through the History section, the first mentioned work that strikes me as interesting is the graph formalism by Frank Harari that “produced the fundamental theorem of this theory”.
Conveniently, Wikipedia &lt;a href=&quot;https://en.wikipedia.org/wiki/Mathematical_sociology#cite_note-5&quot;&gt;cites the paper in question&lt;/a&gt;- Cartwright, Dorwin &amp;amp; Harary, Frank. (1956). “Structural Balance: A Generalization of Heider’s Theory.” Psychological Review 63:277-293.
I head to my University Library’s website and get the pdf (conveniently, my credentials still work though I’ve graduated a few months ago).&lt;/p&gt;

&lt;p&gt;I go over the first few pages to get the groundwork and then start skipping over things, looking for the central results. I’m rewarded with the section “STATEMENT OF THE PROBLEM” and later, the italicized phrases- the theorems in “SOME THEOREMS ON BALANCE”, terms in “FURTHER CONCEPTS IN THE THEORY OF BALANCE” and so on. I decide this paper is interesting and that I probably want to return and read it properly later.&lt;/p&gt;

&lt;p&gt;As a quick aside, I run a Google scholar search of all the articles that cite “Structural Balance: A Generalization of Heider’s Theory”. There are over 2000 results, ordered roughly by citations (“sort by relevance”), with such a broad search I’m just going to glance at the first two pages. Unsurprisingly the papers lie in a large spectrum, from psychology to natural language processing, but some results strike me as useful- J Scott’s “Social Network Analysis”, the first result, seems like a great survey paper. “Network Analysis” by D Knoke and JH Kuklinski, “Structural equivalence of individuals in social networks” by F Lorrain and HC White, and “From Factors to Actors: Computational Sociology and Agent-Based Modeling” by MW Macy and R Willer look like they may belong to my reading list too. I’m sure there’s a lot more, varied content to be found on later search pages, but I want to return to Wikipedia for now.&lt;/p&gt;

&lt;p&gt;Next Wikipedia talks of progress on mathematical process, and presents &lt;a href=&quot;https://en.wikipedia.org/wiki/Mathematical_sociology#cite_note-6&quot;&gt;The Human Group&lt;/a&gt; as an example. I once again look this up, it seems to be a book. I can’t find a version I can access for free online, so at the moment I’ll pass. Maybe I’ll come back and look at works that cite it instead later.&lt;/p&gt;

&lt;p&gt;Moving on, I go through the “Further Developments” section, and can apply the same process to identify the papers that were seminal. I’m not actually going to that yet because I’m feeling lazy. I rationalize that the works in question would still only historical context relative to current research.
The next section tells me of “such central journals as &lt;a href=&quot;https://www.journals.uchicago.edu/toc/ajs/current&quot;&gt;The American Journal of Sociology&lt;/a&gt; of Sociology and &lt;a href=&quot;https://journals.sagepub.com/home/asr&quot;&gt;The American Sociological Review&lt;/a&gt;” and most of all, “&lt;a href=&quot;https://www.tandfonline.com/loi/gmas20&quot;&gt;The Journal of Mathematical Sociology&lt;/a&gt;”. This is very useful information, I can peruse the abstracts of the more recent volumes and get a sense of what’s going on in the field currently. If there are particularly intriguing abstracts that make no sense to me, I can “&lt;a href=&quot;#citationTree&quot;&gt;climb up the citation tree&lt;/a&gt;” till I understand them. But I refrain from doing that just yet, because Wikipedia’s next section, “&lt;a href=&quot;https://en.wikipedia.org/wiki/Mathematical_sociology#Research_programs&quot;&gt;Research programs&lt;/a&gt;” gives me the perfect way to identify an organized set of papers to get an overview of the more important threads of study on the field.&lt;/p&gt;

&lt;p&gt;I will do this analysis for only one of the 9 “programs” described, leaving the rest of the analysis as an &lt;a href=&quot;https://aditink.github.io/satisfying/&quot;&gt;exercise to the reader&lt;/a&gt;.
So my task is now to look at Structuralism. The first thing wikipedia mentions is a book, Chains of Opportunity: System Models of Mobility in Organizations. However a Google scholar search revelas the same author published an article the same year, so I’m guessing it’s closely related. A look at the article confirms the hypothesis.
The process is repeated for the rest of the papers/articles mentioned in the paragraph.
I now proceed to find more recent citations of these papers in google scholar, as I had done with “Structural Balance: A Generalization of Heider’s Theory”. I identify papers with particularly high citation counts and add them to my reading list.
I now have a decent number of papers, but given that I have finite time, I pick and choose how much of each I want to read with the objective of grasping the central idea.
At the end, I think I have a rough idea of what the discipline is about.&lt;/p&gt;

&lt;p&gt;The subject of this case study turned out to be unusual in that I could rely on a central guiding resource, Wikipedia. This will often not be the case, nevertheless I ended up using ideas and methods that would allow you to survey the field regardless.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;We need a starting point.&lt;/p&gt;

    &lt;p&gt;There are many ways to find starting points- a google search that leads you to a survey paper or a textbook, or a summary page for example. In each of these cases you can apply the method of looking up the papers corresponding to the introduction of the important ideas mentioned, as we did here. Or perhaps you know of the important journals or conferences in the field, or even of just an important paper, or of a Professor with papers in the area? If this is the case then you go backwards in time, looking at the important papers cited by your starting paper(s), and then the papers cited by them, and so on, till you reach the point where the important foundational ideas of the field are being introduced.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We need to travel from the starting point to the &lt;em&gt;important&lt;/em&gt; papers.&lt;/p&gt;

    &lt;p&gt;Already spoke a bit about how to do this in the previous point, but the idea is that if you read only recent, specialized work, it’ll be dense and jargon-ey, and you probably won’t cover as much conceptual ground. &lt;a name=&quot;treeAnalogy&quot;&gt;&lt;/a&gt;Think of the literature of a field like a &lt;em&gt;tree&lt;/em&gt;. There are root papers that inspire child papers, and the most recent papers are leaf nodes. The objective is to traverse the tree in such a way as to get a decent idea of it’s general shape, and conceptual understanding of its trunk and thickest branches so that you have the context to comprehend even the leaf nodes. You can climb both up and down a tree, so it’s a tree traversal problem. If you start close to the leaves, you go back to an ancestor, and then look at its most important children. If you start at the root, you do the opposite, and look for successor papers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We need to figure out which papers are important and actually read them.&lt;/p&gt;

    &lt;p&gt;Firstly, abstracts and citation counts are good indicators of importance.
 There is bound to be redundancy in your search, so it might make sense to make a quick first pass on papers and then decide which details to read. In fact, depending on your objective, it’s often ok to focus on the exact things you were looking for (central idea, a specific theorem, methodology used, etc) and let the details fade away.
 The art of reading a paper itself is the oft-discussed subject of many great articles. I get the sense that different things work for different people. The pattern that I tend to follow is getting a sense of the question and the skeleton of the answer from the abstract and introduction, and then asking myself if I understand the answer, and which pieces are unclear or not obvious, that I want to read more about. I then go read those sections, which will generally reveal new questions to me, and iterate. 
 Finally, when there’s assumed background knowledge that makes a paper to be incomprehensible, Googling and reading cited work often helps a lot.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Looking out for keywords in the papers you read that will help with the search-engine work&lt;/p&gt;

    &lt;p&gt;This is generally more useful when drilling down on specific information, and can be seen as a tool to drive down and explore a specific branch of the tree. It did not really arise in the example here. But the idea is that when looking for specific information, important papers will generally introduce terminology for the concepts you seek to investigate. Identify these relevant technical terms so that they can serve as keywords.
  Then you could either run a search with these keywords or use them to narrow down and filter through the results that cite the paper.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;climbing-the-citation-tree-&quot;&gt;Climbing the Citation Tree &lt;a name=&quot;citationTree&quot;&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Going back to the &lt;a href=&quot;#treeAnalogy&quot;&gt;tree analogy&lt;/a&gt;, to climb “up” the paper tree, towards the root, look at the papers cited for the fundamental underlying concepts in the work. These papers are the predecessor nodes of the current paper. You look them up and repeat the process as often as required to climb the citation tree. The reverse can also be done- you can descend down the tree by looking at the important articles that cite the current paper using the Power of Modern Search Engines.&lt;/p&gt;

&lt;p&gt;Climbing trees through citations in this fashion can lead to a sampling bias that helps you identify more important papers. The papers that a randomly chosen paper cites would on an average have more citations than the original paper, akin to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Friendship_paradox&quot;&gt;friendship paradox&lt;/a&gt;.&lt;/p&gt;

&lt;!-- ## Finding Specific Information

I'll write this later, but it's really about keywords and more citation tree climbing. --&gt;

&lt;h4 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h4&gt;
&lt;p&gt;A note of thanks to Victor Odouard who pointed out the sampling bias that “climbing the citation tree” entails.&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry></feed>